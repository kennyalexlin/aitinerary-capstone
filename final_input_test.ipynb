{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1186f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OpenRouter API key.\n",
      "Connected to OpenRouter LLM.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# LLM user simulator config\n",
    "\n",
    "# Load .env from the input_handling_extraction directory\n",
    "dotenv_path = os.path.join('input_handling_extraction', '.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "try:\n",
    "    api_key = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "    print(\"Loaded OpenRouter API key.\")\n",
    "except KeyError:\n",
    "    api_key = None\n",
    "    print(\"ERROR: OPENROUTER_API_KEY not found. Check your .env in 'input_handling_extraction'.\")\n",
    "\n",
    "if api_key:\n",
    "    user_llm_client = openai.OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=api_key\n",
    "    )\n",
    "    print(\"Connected to OpenRouter LLM.\")\n",
    "else:\n",
    "    user_llm_client = None\n",
    "\n",
    "USER_LLM_MODEL = \"google/gemini-2.5-flash-lite-preview-06-17\"\n",
    "CHAT_ENDPOINT = \"http://127.0.0.1:8000/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd468247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-driven user\n",
    "\n",
    "def get_llm_user_response(ground_truth: Dict[str, Any], conversation_history: List[Dict[str, str]], persona: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask an LLM to produce a natural user reply based on the goal, persona, and conversation so far.\n",
    "    \"\"\"\n",
    "    if not user_llm_client:\n",
    "        return \"Error: User LLM client not initialized.\"\n",
    "\n",
    "    last_bot_message = conversation_history[-1]['content'] if conversation_history else \"Hello, how can I help you book your flight today?\"\n",
    "\n",
    "    # Persona-specific behavior\n",
    "    if persona == \"direct_simple\":\n",
    "        persona_instruction = (\n",
    "            \"You’re simulating a direct, straightforward user. \"\n",
    "            \"Share exactly one requested piece of flight info at a time from the data below. \"\n",
    "            \"Answer concisely to the bot’s last question, and only add more if asked, or if the question is broad and one more detail is the obvious next step.\"\n",
    "        )\n",
    "    elif persona == \"comprehensive\":\n",
    "        persona_instruction = (\n",
    "            \"You’re simulating a comprehensive user. \"\n",
    "            \"Try to provide as much of your flight info as you can early on. \"\n",
    "            \"Then respond with whatever’s left or clarify as needed. \"\n",
    "            \"If you can anticipate what the bot needs next, include it proactively.\"\n",
    "        )\n",
    "    elif persona == \"ambiguous\":\n",
    "        persona_instruction = (\n",
    "            \"You’re simulating an ambiguous user. \"\n",
    "            \"You’ll give vague info first when the question allows (e.g., ‘next month’, ‘a few people’, ‘around noon’). \"\n",
    "            \"Only get specific when the bot asks directly. \"\n",
    "            \"If no exact date is in the data, use general phrasing like ‘sometime in 2025’.\"\n",
    "        )\n",
    "    elif persona == \"error_prone\":\n",
    "        persona_instruction = (\n",
    "            \"You’re simulating an error-prone user. \"\n",
    "            \"You might hesitate, change your mind, or rephrase something so the bot has to confirm. \"\n",
    "            \"Eventually give the correct info from the data below when asked directly.\"\n",
    "        )\n",
    "    else:\n",
    "        persona_instruction = \"You’re a helpful user simulator. Provide flight info using the data below.\"\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    {persona_instruction}\n",
    "\n",
    "    Required flight info (ground truth):\n",
    "    {json.dumps(ground_truth, indent=2)}\n",
    "\n",
    "    The bot’s latest message:\n",
    "    \"{last_bot_message}\"\n",
    "\n",
    "    Instructions:\n",
    "    1) Given your persona, read the bot’s last message and your data.\n",
    "    2) Reply with a concise, natural message that helps the booking move forward.\n",
    "    3) Don’t invent facts. Stick to the data or the persona’s guidelines.\n",
    "    4) Don’t send an empty reply. If you’re done, say: \"That's all for now, thank you.\"\n",
    "    \"\"\"\n",
    "\n",
    "    messages_for_llm = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    try:\n",
    "        response = user_llm_client.chat.completions.create(\n",
    "            model=USER_LLM_MODEL,\n",
    "            messages=messages_for_llm,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        user_response = response.choices[0].message.content or \"yes\"\n",
    "        return user_response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling user LLM: {e}\")\n",
    "        return \"An error occurred.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values to compare them more reliably\n",
    "def _normalize_value_for_comparison(value: Any) -> Any:\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    if isinstance(value, str):\n",
    "        normalized_str = value.strip()\n",
    "        if normalized_str.lower() in [\"\", \"null\", \"none\"]:\n",
    "            return None\n",
    "        elif normalized_str.lower() == 'true':\n",
    "            return True\n",
    "        elif normalized_str.lower() == 'false':\n",
    "            return False\n",
    "        else:\n",
    "            try:\n",
    "                return float(normalized_str)\n",
    "            except ValueError:\n",
    "                return normalized_str.lower()\n",
    "    return value\n",
    "\n",
    "def run_llm_driven_test(record: Dict[str, Any], test_index: int, persona: str, print_conversation: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Drive a conversation between the LLM-simulated user and the flight bot.\n",
    "    Returns the final extracted info plus evaluation metrics.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session_id = None\n",
    "    final_flight_info = {}\n",
    "    conversation_history = []\n",
    "    last_bot_messages_for_repetition = []\n",
    "    REPETITION_THRESHOLD = 3\n",
    "    MAX_TURNS = 9\n",
    "    username = f\"testuser_{test_index}_{persona}\"\n",
    "\n",
    "    fields_expected_from_extractor = [\n",
    "        \"departure_city\", \"arrival_city\", \"departure_date\", \"passengers\"\n",
    "    ]\n",
    "\n",
    "    # Prepare ground truth in the same shape the extractor outputs for a fair comparison\n",
    "    def _get_expected_state_for_extractor_comparison(gt_record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        expected_state = {}\n",
    "        for key in fields_expected_from_extractor:\n",
    "            value = gt_record.get(key)\n",
    "            if key == \"passengers\" and (value is None or value == 0):\n",
    "                total_passengers = (gt_record.get(\"adult_passengers\", 0) or 0) + \\\n",
    "                                   (gt_record.get(\"child_passengers\", 0) or 0) + \\\n",
    "                                   (gt_record.get(\"infant_passengers\", 0) or 0)\n",
    "                expected_state[key] = total_passengers if total_passengers > 0 else None\n",
    "            else:\n",
    "                expected_state[key] = value\n",
    "        return {k: _normalize_value_for_comparison(v) for k, v in expected_state.items()}\n",
    "\n",
    "    # Check whether the extracted info matches the ground truth for all evaluated fields\n",
    "    def _is_info_complete_and_correct(current_extracted_info: Dict[str, Any], ground_truth_for_comparison: Dict[str, Any]) -> bool:\n",
    "        for key in fields_expected_from_extractor:\n",
    "            expected_val = ground_truth_for_comparison.get(key)\n",
    "            extracted_val = _normalize_value_for_comparison(current_extracted_info.get(key))\n",
    "            if expected_val != extracted_val:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        requests.post(f\"{CHAT_ENDPOINT.replace('/chat', '')}/users/register\", json={\"username\": username, \"email\": f\"{username}@example.com\"})\n",
    "        login_response = requests.post(f\"{CHAT_ENDPOINT.replace('/chat', '')}/users/login\", params={\"username\": username})\n",
    "        login_response.raise_for_status()\n",
    "        if print_conversation:\n",
    "            print(f\"Signed in as '{username}'.\")\n",
    "    except requests.RequestException as e:\n",
    "        error_msg = f\"Error setting up user '{username}': {e}\"\n",
    "        if print_conversation:\n",
    "            print(error_msg)\n",
    "        return {\"error\": error_msg, \"metrics\": {\"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"conversation_length\": 0, \"true_positives\": 0, \"false_positives\": 0, \"false_negatives\": 0, \"expected_for_evaluation\": _get_expected_state_for_extractor_comparison(record), \"extracted_for_evaluation\": {}}, \"final_flight_info\": {}}\n",
    "\n",
    "    try:\n",
    "        response = session.post(CHAT_ENDPOINT, json={\"role\": \"user\", \"content\": \"Hi!\", \"session_id\": None, \"username\": username}, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        session_id = data[\"session_id\"]\n",
    "        bot_response = data['response']\n",
    "        final_flight_info = data.get(\"flight_info\", {})\n",
    "        if print_conversation:\n",
    "            print(f\"BOT: {bot_response}\")\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "        last_bot_messages_for_repetition.append(bot_response)\n",
    "    except requests.RequestException as e:\n",
    "        error_msg = f\"Error starting session for user {username}: {e}\"\n",
    "        if print_conversation:\n",
    "            print(error_msg)\n",
    "        return {\"error\": error_msg, \"metrics\": {\"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"conversation_length\": 0, \"true_positives\": 0, \"false_positives\": 0, \"false_negatives\": 0, \"expected_for_evaluation\": _get_expected_state_for_extractor_comparison(record), \"extracted_for_evaluation\": {}}, \"final_flight_info\": {}}\n",
    "\n",
    "    expected_ground_truth_for_extractor_comparison = _get_expected_state_for_extractor_comparison(record)\n",
    "\n",
    "    for turn in range(MAX_TURNS):\n",
    "        # Stop if the bot repeats itself N times\n",
    "        if len(last_bot_messages_for_repetition) == REPETITION_THRESHOLD and \\\n",
    "           all(msg == last_bot_messages_for_repetition[0] for msg in last_bot_messages_for_repetition):\n",
    "            if print_conversation:\n",
    "                print(f\"Bot repeated the same message {REPETITION_THRESHOLD} times, ending the conversation.\")\n",
    "            break\n",
    "\n",
    "        # Stop if the bot explicitly confirms a booking\n",
    "        final_booking_confirmation_phrases = [\n",
    "            \"booking confirmed\", \"your booking is complete\", \"your e-ticket and itinerary will be sent\"\n",
    "        ]\n",
    "        if conversation_history and any(phrase.lower() in conversation_history[-1]['content'].lower() for phrase in final_booking_confirmation_phrases):\n",
    "            if print_conversation:\n",
    "                print(\"Booking appears confirmed—ending the test.\")\n",
    "            break\n",
    "\n",
    "        # Stop if we have the core info and the bot starts taking action\n",
    "        essential_fields_for_action_trigger = fields_expected_from_extractor\n",
    "        is_essential_info_present = all(_normalize_value_for_comparison(final_flight_info.get(key)) is not None for key in essential_fields_for_action_trigger)\n",
    "\n",
    "        bot_action_trigger_phrases = [\n",
    "            \"I'll now search for\", \"searching for flights\", \"pull up the results\",\n",
    "            \"check for available flights\", \"here’s your complete booking request\",\n",
    "            \"Here's your confirmed booking summary\", \"proceed with booking this flight\",\n",
    "            \"here’s what I’ve found for your trip\", \"here are some simulated example options\"\n",
    "        ]\n",
    "\n",
    "        if is_essential_info_present and \\\n",
    "           conversation_history and \\\n",
    "           any(phrase.lower() in conversation_history[-1]['content'].lower() for phrase in bot_action_trigger_phrases):\n",
    "            if print_conversation:\n",
    "                print(\"Bot has enough info and is moving to the next step—ending the test.\")\n",
    "            break\n",
    "\n",
    "        # Next user turn\n",
    "        user_response = get_llm_user_response(record, conversation_history, persona)\n",
    "\n",
    "        if \"error\" in user_response.lower() or \"an error occurred.\" in user_response.lower():\n",
    "            if print_conversation:\n",
    "                print(f\"User (LLM) couldn't respond: {user_response}\")\n",
    "            break\n",
    "\n",
    "        if print_conversation:\n",
    "            print(f\"USER (LLM): {user_response}\")\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_response})\n",
    "\n",
    "        try:\n",
    "            response = session.post(CHAT_ENDPOINT, json={\"role\": \"user\", \"content\": user_response, \"session_id\": session_id, \"username\": username}, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            bot_response = data['response']\n",
    "            final_flight_info = data.get(\"flight_info\", {})\n",
    "            if print_conversation:\n",
    "                print(f\"BOT: {bot_response}\")\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "\n",
    "            last_bot_messages_for_repetition.append(bot_response)\n",
    "            if len(last_bot_messages_for_repetition) > REPETITION_THRESHOLD:\n",
    "                last_bot_messages_for_repetition.pop(0)\n",
    "        except requests.RequestException as e:\n",
    "            error_msg = f\"Error during conversation for user {username}: {e}\"\n",
    "            if print_conversation:\n",
    "                print(error_msg)\n",
    "            return {\"error\": error_msg, \"metrics\": {\"precision\": 0, \"recall\": 0, \"f1_score\": 0, \"conversation_length\": len(conversation_history), \"true_positives\": 0, \"false_positives\": 0, \"false_negatives\": 0, \"expected_for_evaluation\": expected_ground_truth_for_extractor_comparison, \"extracted_for_evaluation\": {}}, \"final_flight_info\": final_flight_info}\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    if print_conversation:\n",
    "        print(f\"\\nConversation for {username} ended after {len(conversation_history)} messages.\")\n",
    "\n",
    "    metrics = calculate_slot_metrics(expected_ground_truth_for_extractor_comparison, final_flight_info, fields_expected_from_extractor, len(conversation_history))\n",
    "\n",
    "    return {\"final_flight_info\": final_flight_info, \"metrics\": metrics}\n",
    "\n",
    "def calculate_slot_metrics(expected_data: Dict[str, Any], extracted_data: Dict[str, Any], fields_to_evaluate: List[str], conversation_length: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute slot-filling precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    extracted_data_normalized_for_metrics = {k: _normalize_value_for_comparison(extracted_data.get(k)) for k in fields_to_evaluate}\n",
    "\n",
    "    for field in fields_to_evaluate:\n",
    "        expected_val = expected_data.get(field)\n",
    "        extracted_val = extracted_data_normalized_for_metrics.get(field)\n",
    "\n",
    "        if expected_val is not None:\n",
    "            if extracted_val == expected_val:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "        else:\n",
    "            if extracted_val is not None:\n",
    "                false_positives += 1\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": round(precision, 4),\n",
    "        \"recall\": round(recall, 4),\n",
    "        \"f1_score\": round(f1_score, 4),\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"conversation_length\": conversation_length,\n",
    "        \"expected_for_evaluation\": expected_data,\n",
    "        \"extracted_for_evaluation\": extracted_data_normalized_for_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single test runner\n",
    "\n",
    "TEST_INDEX = 0\n",
    "SELECTED_PERSONA = \"comprehensive\"  # options: \"direct_simple\", \"comprehensive\", \"ambiguous\", \"error_prone\"\n",
    "\n",
    "if 'ground_truth_data' not in globals() or not ground_truth_data:\n",
    "    try:\n",
    "        with open(\"flight_test_data.json\", 'r') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "        print(f\"Loaded {len(ground_truth_data)} records from 'flight_test_data.json'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load test data: {e}\")\n",
    "        ground_truth_data = []\n",
    "\n",
    "if not user_llm_client:\n",
    "    print(\"Can't run: LLM client isn't configured (missing API key).\")\n",
    "elif not ground_truth_data:\n",
    "    print(\"Can't run: Ground truth data isn't loaded.\")\n",
    "elif TEST_INDEX >= len(ground_truth_data):\n",
    "    print(f\"Can't run: test index {TEST_INDEX} is out of bounds (dataset has {len(ground_truth_data)} records).\")\n",
    "else:\n",
    "    record = ground_truth_data[TEST_INDEX]\n",
    "\n",
    "    print(f\"\\n\\nRunning single LLM-driven test (index {TEST_INDEX}, persona: {SELECTED_PERSONA})\\n\")\n",
    "    print(\"Ground truth for this test\")\n",
    "    print(json.dumps(record, indent=2))\n",
    "    print(\"\\nStarting conversation\\n\")\n",
    "\n",
    "    test_result = run_llm_driven_test(record, TEST_INDEX, SELECTED_PERSONA, print_conversation=True)\n",
    "\n",
    "    if \"error\" in test_result:\n",
    "        print(f\"Result: Error - {test_result['error']}\")\n",
    "        print(f\"Metrics at time of failure: {json.dumps(test_result.get('metrics', {}), indent=2)}\")\n",
    "    else:\n",
    "        metrics = test_result[\"metrics\"]\n",
    "        print(f\"\\nFinal test analysis (index {TEST_INDEX}, persona: {SELECTED_PERSONA})\\n\")\n",
    "\n",
    "        expected_data_for_comparison = metrics['expected_for_evaluation']\n",
    "        extracted_data_for_comparison = metrics['extracted_for_evaluation']\n",
    "        is_perfect_match = (metrics['f1_score'] == 1.0 and metrics['false_positives'] == 0)\n",
    "        verdict = 'PASS' if is_perfect_match else 'FAIL'\n",
    "\n",
    "        print(\"\\nExpected data (ground truth)\")\n",
    "        print(json.dumps(expected_data_for_comparison, indent=2))\n",
    "        print(\"\\nExtracted data (from conversation)\")\n",
    "        print(json.dumps(extracted_data_for_comparison, indent=2))\n",
    "        print(f\"\\nResult: {verdict}\")\n",
    "\n",
    "        if not is_perfect_match:\n",
    "            print(\"\\nMismatches:\")\n",
    "            for key in metrics['expected_for_evaluation'].keys():\n",
    "                expected_value = expected_data_for_comparison.get(key)\n",
    "                extracted_value = extracted_data_for_comparison.get(key)\n",
    "                if expected_value != extracted_value:\n",
    "                    print(f\"  - Field: '{key}' | Expected: {expected_value} | Extracted: {extracted_value}\")\n",
    "\n",
    "        print(f\"\\nMetrics:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Conversation Length (messages): {metrics['conversation_length']}\")\n",
    "        print(f\"  True Positives: {metrics['true_positives']}\")\n",
    "        print(f\"  False Positives: {metrics['false_positives']}\")\n",
    "        print(f\"  False Negatives: {metrics['false_negatives']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch test runner\n",
    "\n",
    "# Runs a slice of test cases for multiple personas and writes results to CSV.\n",
    "\n",
    "import os\n",
    "\n",
    "TEST_CASE_RANGE_STR = \"0:20\"\n",
    "PERSONAS_TO_TEST = [\"direct_simple\", \"comprehensive\", \"ambiguous\", \"error_prone\"]\n",
    "RESULTS_CSV_FILE = \"llm_test_results.csv\"\n",
    "\n",
    "if 'ground_truth_data' not in globals() or not ground_truth_data:\n",
    "    try:\n",
    "        with open(\"flight_test_data.json\", 'r') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "        print(f\"Loaded {len(ground_truth_data)} records from 'flight_test_data.json'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load test data: {e}\")\n",
    "        ground_truth_data = []\n",
    "\n",
    "if not user_llm_client:\n",
    "    print(\"Can't run tests: LLM client isn't configured (missing API key).\")\n",
    "elif not ground_truth_data:\n",
    "    print(\"Can't run tests: Ground truth data isn't loaded.\")\n",
    "else:\n",
    "    # Load existing results if the CSV file exists\n",
    "    if os.path.exists(RESULTS_CSV_FILE):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(RESULTS_CSV_FILE)\n",
    "            results_summary = existing_df.to_dict(orient='records')\n",
    "            print(f\"Loaded {len(results_summary)} previous results from '{RESULTS_CSV_FILE}'.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Existing results file '{RESULTS_CSV_FILE}' is empty. Starting fresh.\")\n",
    "            results_summary = []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing results from '{RESULTS_CSV_FILE}': {e}. Starting fresh.\")\n",
    "            results_summary = []\n",
    "    else:\n",
    "        results_summary = []\n",
    "        print(\"No existing results found—starting fresh.\")\n",
    "\n",
    "    fields_expected_from_extractor = [\n",
    "        \"departure_city\", \"arrival_city\", \"departure_date\", \"passengers\"\n",
    "    ]\n",
    "\n",
    "    # Determine the actual range of test indices to run\n",
    "    if TEST_CASE_RANGE_STR.lower() == \"all\":\n",
    "        start_index = 0\n",
    "        end_index = len(ground_truth_data)\n",
    "    else:\n",
    "        try:\n",
    "            parts = TEST_CASE_RANGE_STR.split(':')\n",
    "            start_part = parts[0].strip()\n",
    "            end_part = parts[1].strip()\n",
    "            start_index = int(start_part) if start_part else 0\n",
    "            end_index = int(end_part) if end_part else len(ground_truth_data)\n",
    "            # Ensure indices are within bounds\n",
    "            start_index = max(0, min(start_index, len(ground_truth_data)))\n",
    "            end_index = max(0, min(end_index, len(ground_truth_data)))\n",
    "            if start_index >= end_index:\n",
    "                print(f\"Invalid test case range '{TEST_CASE_RANGE_STR}'. No tests will run.\")\n",
    "                start_index = 0\n",
    "                end_index = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't parse test case range '{TEST_CASE_RANGE_STR}': {e}. Running all tests instead.\")\n",
    "            start_index = 0\n",
    "            end_index = len(ground_truth_data)\n",
    "\n",
    "    # Filter ground_truth_data to the specified range\n",
    "    test_cases_to_run = ground_truth_data[start_index:end_index]\n",
    "    total_test_cases_in_range = len(test_cases_to_run)\n",
    "\n",
    "    # Initialize overall metrics accumulators per persona\n",
    "    overall_metrics_by_persona = {\n",
    "        p: {'tp': 0, 'fp': 0, 'fn': 0, 'length': 0, 'tests_run': 0} for p in PERSONAS_TO_TEST\n",
    "    }\n",
    "\n",
    "    total_tests_run_overall = 0\n",
    "\n",
    "    for persona_name in PERSONAS_TO_TEST:\n",
    "        print(f\"\\nRunning tests for persona: {persona_name}\")\n",
    "        if total_test_cases_in_range == 0:\n",
    "            print(\"  No test cases to run for this range.\")\n",
    "            continue\n",
    "\n",
    "        for i, record in enumerate(test_cases_to_run):\n",
    "            original_test_index = start_index + i\n",
    "\n",
    "            # Check if this test (persona + index) already exists and isn't an error\n",
    "            already_run = False\n",
    "            for existing_result in results_summary:\n",
    "                if (existing_result.get('Persona') == persona_name and\n",
    "                    existing_result.get('Index') == original_test_index and\n",
    "                    existing_result.get('Verdict') != 'ERROR'):\n",
    "                    already_run = True\n",
    "                    if 'TP' in existing_result:\n",
    "                        overall_metrics_by_persona[persona_name]['tp'] += existing_result['TP']\n",
    "                        overall_metrics_by_persona[persona_name]['fp'] += existing_result['FP']\n",
    "                        overall_metrics_by_persona[persona_name]['fn'] += existing_result['FN']\n",
    "                        overall_metrics_by_persona[persona_name]['length'] += existing_result['Length']\n",
    "                        overall_metrics_by_persona[persona_name]['tests_run'] += 1\n",
    "                        total_tests_run_overall += 1\n",
    "                    break\n",
    "\n",
    "            if already_run:\n",
    "                print(f\"  Skipping test case {i+1}/{total_test_cases_in_range} (original index: {original_test_index}) for {persona_name} (already completed).\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Running test case {i+1}/{total_test_cases_in_range} (original index: {original_test_index}) for {persona_name}...\")\n",
    "            test_result = run_llm_driven_test(record, original_test_index, persona_name, print_conversation=False)\n",
    "\n",
    "            metrics = test_result.get('metrics', {})\n",
    "\n",
    "            # Accumulate overall metrics for this persona (newly run tests)\n",
    "            overall_metrics_by_persona[persona_name]['tp'] += metrics.get('true_positives', 0)\n",
    "            overall_metrics_by_persona[persona_name]['fp'] += metrics.get('false_positives', 0)\n",
    "            overall_metrics_by_persona[persona_name]['fn'] += metrics.get('false_negatives', 0)\n",
    "            overall_metrics_by_persona[persona_name]['length'] += metrics.get('conversation_length', 0)\n",
    "            overall_metrics_by_persona[persona_name]['tests_run'] += 1\n",
    "            total_tests_run_overall += 1\n",
    "\n",
    "            verdict = 'ERROR' if \"error\" in test_result else ('PASS' if (metrics.get('f1_score', 0) == 1.0 and metrics.get('false_positives', 0) == 0) else 'FAIL')\n",
    "\n",
    "            result_row = {\n",
    "                'Persona': persona_name,\n",
    "                'Index': original_test_index,\n",
    "                'Verdict': verdict,\n",
    "                'Precision': metrics.get('precision', 0.0),\n",
    "                'Recall': metrics.get('recall', 0.0),\n",
    "                'F1-Score': metrics.get('f1_score', 0.0),\n",
    "                'Length': metrics.get('conversation_length', 0),\n",
    "                'TP': metrics.get('true_positives', 0),\n",
    "                'FP': metrics.get('false_positives', 0),\n",
    "                'FN': metrics.get('false_negatives', 0),\n",
    "                'Expected': json.dumps(metrics.get('expected_for_evaluation', {})),\n",
    "                'Extracted': json.dumps(metrics.get('extracted_for_evaluation', {}))\n",
    "            }\n",
    "            results_summary.append(result_row)\n",
    "\n",
    "            # Save results after each test\n",
    "            current_summary_df = pd.DataFrame(results_summary)\n",
    "            current_summary_df.to_csv(RESULTS_CSV_FILE, index=False)\n",
    "\n",
    "    print(\"\\n\\nBatch test summary\")\n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    if not summary_df.empty:\n",
    "        print(summary_df.to_string())\n",
    "    else:\n",
    "        print(\"No tests were run or loaded.\")\n",
    "\n",
    "    print(f\"\\nOverall performance across selected tests (total {total_tests_run_overall} runs):\\n\")\n",
    "    if total_tests_run_overall == 0:\n",
    "        print(\"  No data to calculate overall metrics.\")\n",
    "    else:\n",
    "        for p_name, p_metrics in overall_metrics_by_persona.items():\n",
    "            if p_metrics['tests_run'] > 0:\n",
    "                overall_precision = p_metrics['tp'] / (p_metrics['tp'] + p_metrics['fp']) if (p_metrics['tp'] + p_metrics['fp']) > 0 else 0.0\n",
    "                overall_recall = p_metrics['tp'] / (p_metrics['tp'] + p_metrics['fn']) if (p_metrics['tp'] + p_metrics['fn']) > 0 else 0.0\n",
    "                overall_f1_score = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
    "                avg_length = p_metrics['length'] / p_metrics['tests_run']\n",
    "\n",
    "                persona_summary_df = summary_df[summary_df['Persona'] == p_name]\n",
    "                persona_pass_count = persona_summary_df[persona_summary_df['Verdict'] == 'PASS'].shape[0]\n",
    "                persona_fail_count = persona_summary_df[persona_summary_df['Verdict'] == 'FAIL'].shape[0]\n",
    "                persona_error_count = persona_summary_df[persona_summary_df['Verdict'] == 'ERROR'].shape[0]\n",
    "\n",
    "                print(f\"Persona: {p_name}\")\n",
    "                print(f\"  Total tests: {p_metrics['tests_run']}\")\n",
    "                print(f\"  Passed: {persona_pass_count}\")\n",
    "                print(f\"  Failed: {persona_fail_count}\")\n",
    "                print(f\"  Errors: {persona_error_count}\")\n",
    "                print(f\"  Precision: {overall_precision:.4f}\")\n",
    "                print(f\"  Recall: {overall_recall:.4f}\")\n",
    "                print(f\"  F1-Score: {overall_f1_score:.4f}\")\n",
    "                print(f\"  Average conversation length: {avg_length:.2f} messages\")\n",
    "            else:\n",
    "                print(f\"Persona: {p_name}\")\n",
    "                print(\"  No tests run for this persona in the selected range.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
